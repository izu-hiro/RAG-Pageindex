{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/izu-hiro/RAG-Pageindex/blob/main/prototipo_rag/quary.teste.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "id": "353e069ded4a0ab2"
  },
  {
   "metadata": {
    "id": "f8897a5bb9837baa"
   },
   "cell_type": "markdown",
   "source": "Definindo a LLM",
   "id": "c97510c1ed08c7a2"
  },
  {
   "metadata": {
    "collapsed": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b58922740f50bf25",
    "outputId": "ed47b3dd-fd88-4b83-afe0-37e29d83b68e",
    "ExecuteTime": {
     "end_time": "2025-10-08T00:46:19.682765Z",
     "start_time": "2025-10-08T00:45:54.141746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import AsyncOpenAI\n",
    "from getpass import getpass\n",
    "\n",
    "openrouterKey = getpass(\"Insira a sua chave API: \")\n",
    "\n",
    "async def call_llm(prompt, model=\"deepseek/deepseek-chat-v3.1:free\", temperature=0):\n",
    "    client = AsyncOpenAI(base_url=\"https://openrouter.ai/api/v1\", api_key=openrouterKey)\n",
    "    response = await client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ],
   "id": "6ef9931f65af204c",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Conectando com o PageIndex",
   "id": "475a6ba64b65af82"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T00:46:38.374640Z",
     "start_time": "2025-10-08T00:46:22.326798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pageindex import PageIndexClient\n",
    "\n",
    "pi_client = PageIndexClient(api_key=getpass(\"Insira a chave API do Page Index: \"))"
   ],
   "id": "f77ba117effdf3ba",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "id": "4460f387fe7747cb"
   },
   "cell_type": "markdown",
   "source": "Contexto utilizado na geração da resposta",
   "id": "3c4f663712d97325"
  },
  {
   "metadata": {
    "id": "4348da21357ed3fb",
    "ExecuteTime": {
     "end_time": "2025-10-08T00:56:15.553213Z",
     "start_time": "2025-10-08T00:56:15.119100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pypdf import PdfWriter\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "documentos = ['documentos/'+f for f in listdir('documentos') if isfile(join('documentos', f))]\n",
    "\n",
    "merger = PdfWriter()\n",
    "for documento in documentos:\n",
    "    merger.append(documento)\n",
    "merger.write(\"documentoUnido.pdf\")\n",
    "merger.close()\n",
    "\n"
   ],
   "id": "97cc7b8d1177931f",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Enviar arquivo e pegar o ID",
   "id": "38f84fabd2a1b32a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T00:56:21.584243Z",
     "start_time": "2025-10-08T00:56:18.413827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = pi_client.submit_document(\"./documentoUnido.pdf\")\n",
    "doc_id = result[\"doc_id\"]\n",
    "\n",
    "# Get OCR results in page format (default)\n",
    "ocr_result = pi_client.get_ocr(doc_id)\n",
    "if ocr_result.get(\"status\") == \"completed\":\n",
    "    print(\"OCR Results:\", ocr_result.get(\"result\"))\n",
    "\n",
    "'''\n",
    "# Get OCR results in node format\n",
    "ocr_result = pi_client.get_ocr(doc_id, format=\"node\")\n",
    "if ocr_result.get(\"status\") == \"completed\":\n",
    "    print(\"OCR Results:\", ocr_result.get(\"result\"))\n",
    "\n",
    "# Get OCR results in raw format (concatenated markdown)\n",
    "ocr_result = pi_client.get_ocr(doc_id, format=\"raw\")\n",
    "if ocr_result.get(\"status\") == \"completed\":\n",
    "    print(\"Raw Markdown:\", ocr_result.get(\"result\"))\n",
    "'''\n",
    "\n",
    "tree_result = pi_client.get_tree(doc_id)\n",
    "if tree_result.get(\"status\") == \"completed\":\n",
    "    print(\"PageIndex Tree Structure:\", tree_result.get(\"result\"))"
   ],
   "id": "8eade8b36786cb44",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Retrieval",
   "id": "9e9ec63eb603eedf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T00:56:23.536498Z",
     "start_time": "2025-10-08T00:56:23.528901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from time import sleep\n",
    "def retrieval(query):\n",
    "    while True:\n",
    "        if pi_client.is_retrieval_ready(doc_id):\n",
    "            retrieval = pi_client.submit_query(doc_id, query)\n",
    "            retrieval_id = retrieval['retrieval_id']\n",
    "            break\n",
    "        else:\n",
    "            print(\"Document is not ready for retrieval yet\")\n",
    "            sleep(5)\n",
    "\n",
    "    while True:\n",
    "        retrieval_result = pi_client.get_retrieval(retrieval_id)\n",
    "        if retrieval_result.get(\"status\") == \"completed\":\n",
    "            return retrieval_result.get('result')\n",
    "        sleep(1)\n"
   ],
   "id": "aecd3482fb81c425",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "id": "1970d0b0de62b5db"
   },
   "cell_type": "markdown",
   "source": "Geração da resposta",
   "id": "38b2d18e16708c4a"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c172d04976b2ed53",
    "outputId": "25e4848c-0556-4fdb-c807-2c39d27ceeee",
    "ExecuteTime": {
     "end_time": "2025-10-08T00:57:34.234005Z",
     "start_time": "2025-10-08T00:56:25.406667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "perguntas = [\n",
    "    \"Qual é a idade mínima para poder ingressar na fatec?\",\n",
    "    \"Quanto custa pra estudar numa instituicao do centro paula souza?\",\n",
    "    \"Como q funciona o processo de inscrição no vestibulinho da etec?\",\n",
    "    \"Fatec tem programa de mestrado? se sim, qnd começam as aulas do mestrado de 2025?\"\n",
    "]\n",
    "\n",
    "for query in perguntas:\n",
    "    relevant_content = retrieval(query)\n",
    "    print(relevant_content)\n",
    "    answer_prompt = f\"\"\"\n",
    "Responda à pergunta com base apenas no contexto fornecido abaixo.\n",
    "\n",
    "- Interprete a pergunta em **português**, mesmo que ela contenha:\n",
    "  - Gírias ou linguagem informal\n",
    "  - Erros de digitação ou ortografia\n",
    "  - Abreviações ou escrita de forma não convencional\n",
    "  - Estruturas de frase incompletas ou confusas\n",
    "- Se houver ambiguidade, faça a melhor interpretação possível considerando o contexto.\n",
    "- Mantenha a resposta **clara, completa e em português correto**.\n",
    "- Não invente informações que não estejam no contexto.\n",
    "- Use exemplos ou explicações adicionais, se necessário, para garantir que a resposta seja compreendida.\n",
    "\n",
    "\n",
    "    Pergunta do usuário: {query}\n",
    "    Contexto: {relevant_content}\n",
    "\n",
    "    Resposta:\n",
    "    \"\"\"\n",
    "\n",
    "    answer = await call_llm(answer_prompt)\n",
    "\n",
    "    print(\"Pergunta original:\\n\", query)\n",
    "    print(\"\\nResposta gerada (PT):\\n\", answer)\n"
   ],
   "id": "32cd9e458df46f49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document is not ready for retrieval yet\n",
      "Document is not ready for retrieval yet\n",
      "Document is not ready for retrieval yet\n",
      "Document is not ready for retrieval yet\n",
      "Document is not ready for retrieval yet\n",
      "Document is not ready for retrieval yet\n",
      "Document is not ready for retrieval yet\n",
      "Document is not ready for retrieval yet\n",
      "Document is not ready for retrieval yet\n",
      "Document is not ready for retrieval yet\n",
      "Document is not ready for retrieval yet\n",
      "Document is not ready for retrieval yet\n",
      "Document is not ready for retrieval yet\n",
      "Document is not ready for retrieval yet\n",
      "Document is not ready for retrieval yet\n",
      "Document is not ready for retrieval yet\n",
      "Document is not ready for retrieval yet\n",
      "Document is not ready for retrieval yet\n",
      "Document is not ready for retrieval yet\n",
      "Document is not ready for retrieval yet\n",
      "Document is not ready for retrieval yet\n",
      "Document is not ready for retrieval yet\n",
      "Document is not ready for retrieval yet\n",
      "Document is not ready for retrieval yet\n",
      "Document is not ready for retrieval yet\n",
      "Document is not ready for retrieval yet\n",
      "Document is not ready for retrieval yet\n",
      "None\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'message': 'No endpoints found matching your data policy (Free model publication). Configure: https://openrouter.ai/settings/privacy', 'code': 404}}",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNotFoundError\u001B[39m                             Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[16]\u001B[39m\u001B[32m, line 31\u001B[39m\n\u001B[32m     10\u001B[39m     \u001B[38;5;28mprint\u001B[39m(relevant_content)\n\u001B[32m     11\u001B[39m     answer_prompt = \u001B[33mf\u001B[39m\u001B[33m\"\"\"\u001B[39m\n\u001B[32m     12\u001B[39m \u001B[33mResponda à pergunta com base apenas no contexto fornecido abaixo.\u001B[39m\n\u001B[32m     13\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m     28\u001B[39m \u001B[33m    Resposta:\u001B[39m\n\u001B[32m     29\u001B[39m \u001B[33m    \u001B[39m\u001B[33m\"\"\"\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m31\u001B[39m     answer = \u001B[38;5;28;01mawait\u001B[39;00m call_llm(answer_prompt)\n\u001B[32m     33\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mPergunta original:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m, query)\n\u001B[32m     34\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mResposta gerada (PT):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m, answer)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 8\u001B[39m, in \u001B[36mcall_llm\u001B[39m\u001B[34m(prompt, model, temperature)\u001B[39m\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcall_llm\u001B[39m(prompt, model=\u001B[33m\"\u001B[39m\u001B[33mdeepseek/deepseek-chat-v3.1:free\u001B[39m\u001B[33m\"\u001B[39m, temperature=\u001B[32m0\u001B[39m):\n\u001B[32m      7\u001B[39m     client = AsyncOpenAI(base_url=\u001B[33m\"\u001B[39m\u001B[33mhttps://openrouter.ai/api/v1\u001B[39m\u001B[33m\"\u001B[39m, api_key=openrouterKey)\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m     response = \u001B[38;5;28;01mawait\u001B[39;00m client.chat.completions.create(\n\u001B[32m      9\u001B[39m         model=model,\n\u001B[32m     10\u001B[39m         messages=[{\u001B[33m\"\u001B[39m\u001B[33mrole\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33muser\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mcontent\u001B[39m\u001B[33m\"\u001B[39m: prompt}],\n\u001B[32m     11\u001B[39m         temperature=temperature\n\u001B[32m     12\u001B[39m     )\n\u001B[32m     13\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m response.choices[\u001B[32m0\u001B[39m].message.content.strip()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/Fatec/GrupoDeEstudos/prototipo/venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py:2585\u001B[39m, in \u001B[36mAsyncCompletions.create\u001B[39m\u001B[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001B[39m\n\u001B[32m   2539\u001B[39m \u001B[38;5;129m@required_args\u001B[39m([\u001B[33m\"\u001B[39m\u001B[33mmessages\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m], [\u001B[33m\"\u001B[39m\u001B[33mmessages\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mstream\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m   2540\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcreate\u001B[39m(\n\u001B[32m   2541\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   2582\u001B[39m     timeout: \u001B[38;5;28mfloat\u001B[39m | httpx.Timeout | \u001B[38;5;28;01mNone\u001B[39;00m | NotGiven = not_given,\n\u001B[32m   2583\u001B[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001B[32m   2584\u001B[39m     validate_response_format(response_format)\n\u001B[32m-> \u001B[39m\u001B[32m2585\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._post(\n\u001B[32m   2586\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m/chat/completions\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   2587\u001B[39m         body=\u001B[38;5;28;01mawait\u001B[39;00m async_maybe_transform(\n\u001B[32m   2588\u001B[39m             {\n\u001B[32m   2589\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mmessages\u001B[39m\u001B[33m\"\u001B[39m: messages,\n\u001B[32m   2590\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m: model,\n\u001B[32m   2591\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33maudio\u001B[39m\u001B[33m\"\u001B[39m: audio,\n\u001B[32m   2592\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mfrequency_penalty\u001B[39m\u001B[33m\"\u001B[39m: frequency_penalty,\n\u001B[32m   2593\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mfunction_call\u001B[39m\u001B[33m\"\u001B[39m: function_call,\n\u001B[32m   2594\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mfunctions\u001B[39m\u001B[33m\"\u001B[39m: functions,\n\u001B[32m   2595\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mlogit_bias\u001B[39m\u001B[33m\"\u001B[39m: logit_bias,\n\u001B[32m   2596\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mlogprobs\u001B[39m\u001B[33m\"\u001B[39m: logprobs,\n\u001B[32m   2597\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mmax_completion_tokens\u001B[39m\u001B[33m\"\u001B[39m: max_completion_tokens,\n\u001B[32m   2598\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mmax_tokens\u001B[39m\u001B[33m\"\u001B[39m: max_tokens,\n\u001B[32m   2599\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: metadata,\n\u001B[32m   2600\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mmodalities\u001B[39m\u001B[33m\"\u001B[39m: modalities,\n\u001B[32m   2601\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mn\u001B[39m\u001B[33m\"\u001B[39m: n,\n\u001B[32m   2602\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mparallel_tool_calls\u001B[39m\u001B[33m\"\u001B[39m: parallel_tool_calls,\n\u001B[32m   2603\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mprediction\u001B[39m\u001B[33m\"\u001B[39m: prediction,\n\u001B[32m   2604\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mpresence_penalty\u001B[39m\u001B[33m\"\u001B[39m: presence_penalty,\n\u001B[32m   2605\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mprompt_cache_key\u001B[39m\u001B[33m\"\u001B[39m: prompt_cache_key,\n\u001B[32m   2606\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mreasoning_effort\u001B[39m\u001B[33m\"\u001B[39m: reasoning_effort,\n\u001B[32m   2607\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mresponse_format\u001B[39m\u001B[33m\"\u001B[39m: response_format,\n\u001B[32m   2608\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33msafety_identifier\u001B[39m\u001B[33m\"\u001B[39m: safety_identifier,\n\u001B[32m   2609\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mseed\u001B[39m\u001B[33m\"\u001B[39m: seed,\n\u001B[32m   2610\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mservice_tier\u001B[39m\u001B[33m\"\u001B[39m: service_tier,\n\u001B[32m   2611\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mstop\u001B[39m\u001B[33m\"\u001B[39m: stop,\n\u001B[32m   2612\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mstore\u001B[39m\u001B[33m\"\u001B[39m: store,\n\u001B[32m   2613\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mstream\u001B[39m\u001B[33m\"\u001B[39m: stream,\n\u001B[32m   2614\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mstream_options\u001B[39m\u001B[33m\"\u001B[39m: stream_options,\n\u001B[32m   2615\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mtemperature\u001B[39m\u001B[33m\"\u001B[39m: temperature,\n\u001B[32m   2616\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mtool_choice\u001B[39m\u001B[33m\"\u001B[39m: tool_choice,\n\u001B[32m   2617\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mtools\u001B[39m\u001B[33m\"\u001B[39m: tools,\n\u001B[32m   2618\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mtop_logprobs\u001B[39m\u001B[33m\"\u001B[39m: top_logprobs,\n\u001B[32m   2619\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mtop_p\u001B[39m\u001B[33m\"\u001B[39m: top_p,\n\u001B[32m   2620\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33muser\u001B[39m\u001B[33m\"\u001B[39m: user,\n\u001B[32m   2621\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mverbosity\u001B[39m\u001B[33m\"\u001B[39m: verbosity,\n\u001B[32m   2622\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mweb_search_options\u001B[39m\u001B[33m\"\u001B[39m: web_search_options,\n\u001B[32m   2623\u001B[39m             },\n\u001B[32m   2624\u001B[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001B[32m   2625\u001B[39m             \u001B[38;5;28;01mif\u001B[39;00m stream\n\u001B[32m   2626\u001B[39m             \u001B[38;5;28;01melse\u001B[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001B[32m   2627\u001B[39m         ),\n\u001B[32m   2628\u001B[39m         options=make_request_options(\n\u001B[32m   2629\u001B[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001B[32m   2630\u001B[39m         ),\n\u001B[32m   2631\u001B[39m         cast_to=ChatCompletion,\n\u001B[32m   2632\u001B[39m         stream=stream \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m   2633\u001B[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001B[32m   2634\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/Fatec/GrupoDeEstudos/prototipo/venv/lib/python3.13/site-packages/openai/_base_client.py:1794\u001B[39m, in \u001B[36mAsyncAPIClient.post\u001B[39m\u001B[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001B[39m\n\u001B[32m   1780\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpost\u001B[39m(\n\u001B[32m   1781\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1782\u001B[39m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1789\u001B[39m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_AsyncStreamT] | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1790\u001B[39m ) -> ResponseT | _AsyncStreamT:\n\u001B[32m   1791\u001B[39m     opts = FinalRequestOptions.construct(\n\u001B[32m   1792\u001B[39m         method=\u001B[33m\"\u001B[39m\u001B[33mpost\u001B[39m\u001B[33m\"\u001B[39m, url=path, json_data=body, files=\u001B[38;5;28;01mawait\u001B[39;00m async_to_httpx_files(files), **options\n\u001B[32m   1793\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m1794\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/Fatec/GrupoDeEstudos/prototipo/venv/lib/python3.13/site-packages/openai/_base_client.py:1594\u001B[39m, in \u001B[36mAsyncAPIClient.request\u001B[39m\u001B[34m(self, cast_to, options, stream, stream_cls)\u001B[39m\n\u001B[32m   1591\u001B[39m             \u001B[38;5;28;01mawait\u001B[39;00m err.response.aread()\n\u001B[32m   1593\u001B[39m         log.debug(\u001B[33m\"\u001B[39m\u001B[33mRe-raising status error\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1594\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m._make_status_error_from_response(err.response) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1596\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m   1598\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[33m\"\u001B[39m\u001B[33mcould not resolve response (should never happen)\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[31mNotFoundError\u001B[39m: Error code: 404 - {'error': {'message': 'No endpoints found matching your data policy (Free model publication). Configure: https://openrouter.ai/settings/privacy', 'code': 404}}"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "include_colab_link": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
