from openai import AsyncOpenAI
from getpass import getpass
from pageindex import PageIndexClient
from pypdf import PdfWriter
from os import listdir
from os.path import isfile, join
from time import sleep


"""rag.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/izu-hiro/RAG-Pageindex/blob/main/rag.ipynb

Definindo a LLM
"""


openrouterKey = getpass("Insira a sua chave API: ")

async def call_llm(prompt, model="openai/gpt-oss-20b:free", temperature=0):
    client = AsyncOpenAI(base_url="https://openrouter.ai/api/v1", api_key=openrouterKey)
    response = await client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        temperature=temperature
    )
    return response.choices[0].message.content.strip()

#"""Conectando com o PageIndex"""

pi_client = PageIndexClient(api_key=input("Insira a chave API do Page Index: "))

#"""Contexto utilizado na geração da resposta"""

documentos = ['documentos/'+f for f in listdir('documentos') if isfile(join('documentos', f))]

merger = PdfWriter()
for documento in documentos:
    merger.append(documento)
merger.write("documentoUnido.pdf")
merger.close()

#"""Enviar arquivo e pegar o ID"""

op = 2
while not op in (0, 1):
    op = int(input("Insira 0 para subir um novo documento ou 1 para inserir a chave de um documento existente: "))
if op == 0:
    result = pi_client.submit_document("./documentoUnido.pdf")
    doc_id = result["doc_id"]
else:
    doc_id = getpass("insira o ID do documento: ")


# Get OCR results in page format (default)

try:
    ocr_result = pi_client.get_ocr(doc_id)
    while True:
        if ocr_result.get("status") == "completed":
            print("OCR Results:", ocr_result.get("result"))
            break
        else:
            sleep(5)
except Exception as e:
    print(f"OCR indisponivel ou nao suportado")
'''

# Get OCR results in node format
ocr_result = pi_client.get_ocr(doc_id, format="node")
if ocr_result.get("status") == "completed":
    print("OCR Results:", ocr_result.get("result"))

 # Get OCR results in raw format (concatenated markdown)
ocr_result = pi_client.get_ocr(doc_id, format="raw")
if ocr_result.get("status") == "completed":
    print("Raw Markdown:", ocr_result.get("result"))
'''

tree_result = pi_client.get_tree(doc_id)
while True:
    if tree_result.get("status") == "completed":
        print("PageIndex Tree Structure:", tree_result.get("result"))
        break
    else:
        sleep(5)

#"""Retrieval"""

def retrieval(query):
    while True:
        if pi_client.is_retrieval_ready(doc_id):
            retrieval = pi_client.submit_query(doc_id, query)
            retrieval_id = retrieval['retrieval_id']
            break
        else:
            print("Document is not ready for retrieval yet")
            sleep(5)

    while True:
        retrieval_result = pi_client.get_retrieval(retrieval_id)
        if retrieval_result.get("status") == "completed":
            return retrieval_result.get('retrieved_nodes')
        sleep(1)

async def executar_rag_stepback(original_query: str):
    print("### INICIANDO DEMONSTRAÇÃO ###\n")

    # Pergunta original 
    print(f"Pergunta Original Específica:\n{original_query}\n")

    # prompt pra llm
    step_back_prompt = f"""
    Dada a seguinte pergunta específica, formule uma pergunta "step-back" mais geral que capture os conceitos fundamentais necessários para respondê-la.

    Pergunta Específica: "{original_query}"

    Pergunta Step-Back Geral:
    """

    print("--- PASSO 1: Gerando a Pergunta ---")

    step_back_question = await call_llm(step_back_prompt)
    print(f"Pergunta Gerada: {step_back_question}\n")

    print("--- PASSO 2: Utilizando o contexto já carregado ---")
    retrieved_context = retrieval(step_back_question)
    print("Contexto recuperado com sucesso.\n")
    print(f"O contexto recuperado foi:\n{retrieved_context}")

    # prompt final 
    final_answer_prompt = f"""
    Você deve responder à "Pergunta Específica" usando o "Contexto" fornecido.
    Para te guiar, primeiro considere a "Pergunta Step-Back Geral" e como o contexto a responde.
    Use esse entendimento para construir uma resposta completa e precisa para a pergunta original.
    Responda somente aquilo que for passado no contexto oferecido. Se o contexto não for suficiente para responder a pergunta, apenas diga ao usuário que não foi possível responder aquela pergunta com as informações que você possui.

    Contexto:
    ---
    {retrieved_context}
    ---

    Pergunta Step-Back Geral: {step_back_question}
    Pergunta Específica: {original_query}

    Resposta Final Detalhada:
    """

    print("--- PASSO 3: Gerando a Resposta Final ---")

    final_answer = await call_llm(final_answer_prompt)
    print("Resposta Gerada:\n")
    print(final_answer)

    return final_answer  
    
