from openai import AsyncOpenAI
from getpass import getpass
from pageindex import PageIndexClient
from pypdf import PdfWriter
from os import listdir
from os.path import isfile, join
from time import sleep
from ollama import chat
from ollama import ChatResponse
import re

"""rag.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/izu-hiro/RAG-Pageindex/blob/main/rag.ipynb

Definindo a LLM
"""


def call_llm(input_content, system_prompt, deep_think = True, print_log = True):
    response: ChatResponse = chat(model='deepseek-r1:1.5b', messages=[
        {'role': 'system', 'content': system_prompt},
        {'role': 'user', 'content': input_content}
    ])
    response_text = response['message']['content']
    if print_log: print(response_text)

    think_texts = re.findall(r'<think>(.*?)</think>', response_text, flags=re.DOTALL)
    think_texts = "\n\n".join(think_texts).strip()
    clean_response = re.sub(r'<think>.*?</think>', '', response_text, flags=re.DOTALL).strip()

    return clean_response if not deep_think else (clean_response, think_texts)

#"""Conectando com o PageIndex"""

pi_client = PageIndexClient(api_key=input("Insira a chave API do Page Index: "))

#"""Contexto utilizado na geração da resposta"""

documentos = ['documentos/'+f for f in listdir('documentos') if isfile(join('documentos', f))]

merger = PdfWriter()
for documento in documentos:
    merger.append(documento)
merger.write("documentoUnido.pdf")
merger.close()

#"""Enviar arquivo e pegar o ID"""

op = 2
while not op in (0, 1):
    op = int(input("Insira 0 para subir um novo documento ou 1 para inserir a chave de um documento existente: "))
if op == 0:
    result = pi_client.submit_document("./documentoUnido.pdf")
    doc_id = result["doc_id"]
else:
    doc_id = input("insira o ID do documento: ")


# Get OCR results in page format (default)

try:
    ocr_result = pi_client.get_ocr(doc_id)
    while True:
        if ocr_result.get("status") == "completed":
            print("OCR Results:", ocr_result.get("result"))
            break
        else:
            sleep(5)
except Exception as e:
    print(f"OCR indisponivel ou nao suportado")
'''

# Get OCR results in node format
ocr_result = pi_client.get_ocr(doc_id, format="node")
if ocr_result.get("status") == "completed":
    print("OCR Results:", ocr_result.get("result"))

 # Get OCR results in raw format (concatenated markdown)
ocr_result = pi_client.get_ocr(doc_id, format="raw")
if ocr_result.get("status") == "completed":
    print("Raw Markdown:", ocr_result.get("result"))
'''

tree_result = pi_client.get_tree(doc_id)
while True:
    if tree_result.get("status") == "completed":
        print("PageIndex Tree Structure:", tree_result.get("result"))
        break
    else:
        sleep(5)

#"""Retrieval"""

def retrieval(query):
    while True:
        if pi_client.is_retrieval_ready(doc_id):
            retrieval = pi_client.submit_query(doc_id, query)
            retrieval_id = retrieval['retrieval_id']
            break
        else:
            print("Document is not ready for retrieval yet")
            sleep(5)

    while True:
        retrieval_result = pi_client.get_retrieval(retrieval_id)
        if retrieval_result.get("status") == "completed":
            return retrieval_result.get('retrieved_nodes')
        sleep(1)

print("### INICIANDO DEMONSTRAÇÃO ###\n")

# Pergunta original
original_query = "Qual é a idade mínima para ingressar na Fatec Araraquara?"

# prompt pra llm
step_back_prompt = 'Dada a seguinte pergunta específica, formule uma pergunta "step-back" mais geral que capture os conceitos fundamentais necessários para respondê-la.'

print("--- PASSO 1: Gerando a Pergunta ---")

step_back_question = call_llm(original_query, step_back_prompt)
print(f"Pergunta Gerada: {step_back_question}\n")

print("--- PASSO 2: Utilizando o contexto já carregado ---")
retrieved_context = retrieval(step_back_question)
print("Contexto recuperado com sucesso.\n")
print(f"O contexto recuperado foi:\n{retrieved_context}")

# prompt final
final_answer_prompt = f"""
Você deve responder à "Pergunta Específica" usando o "Contexto" fornecido.
Para te guiar, primeiro considere a "Pergunta Step-Back Geral" e como o contexto a responde.
Use esse entendimento para construir uma resposta completa e precisa para a pergunta original.
Responda somente aquilo que for passado no contexto oferecido. Se o contexto não for suficiente para responder a pergunta, apenas diga ao usuário que não foi possível responder aquela pergunta com as informações que você possui.

Contexto:
---
{retrieved_context}
---
"""
queries = f"""
Pergunta Step-Back Geral: {step_back_question}
Pergunta Específica: {original_query}
"""

print("--- PASSO 3: Gerando a Resposta Final ---")

final_answer = call_llm(queries, final_answer_prompt)
print("Resposta Gerada:\n")
print(final_answer)
    
